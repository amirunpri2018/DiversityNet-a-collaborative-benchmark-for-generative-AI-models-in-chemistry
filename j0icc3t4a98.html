<div>for <a href="https://en.wikipedia.org/wiki/Peptide" target="_blank">peptides</a> (<a href="http://pubs.acs.org/doi/10.1021/acs.jcim.7b00414" target="_blank">ETH Zurich 3</a>). However, I think it is better to keep those non-chemistry tasks for separate benchmarks: DiversityNet-genetics and DiversityNet-peptides.&nbsp;</div><h1 data-label="714352" class="ltx_title_section">Generative models</h1><div>It’s good to start with models already tried in the literature:</div><ul><li><a href="http://kvfrans.com/variational-autoencoders-explained/" target="_blank">Variational auto-encoder</a>: <a href="http://pubs.acs.org/doi/abs/10.1021/acscentsci.7b00572" target="_blank">Harvard 1</a>, <a href="https://arxiv.org/abs/1703.01925" target="_blank">Alan Turing Institute</a>, <a href="https://arxiv.org/abs/1711.07839" target="_blank">AstraZeneca 3</a>, <a href="https://openreview.net/forum?id=SyqShMZRb" target="_blank">Georgia Tech</a>,&nbsp;<a href="http://onlinelibrary.wiley.com/doi/10.1002/minf.201700133/full" target="_blank">Denmark Tech</a>&nbsp;(use&nbsp;<a href="https://en.wikipedia.org/wiki/Sci-Hub" target="_blank">Sci-Hub</a>&nbsp;for the paywall)</li><li><a href="https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4" target="_blank">Adversarial auto-encoder</a>: <a href="http://www.oncotarget.com/index.php?journal=oncotarget&amp;page=article&amp;op=view&amp;path%5B%5D=14073&amp;path%5B%5D=44886" target="_blank">In SilicoMedicine 1</a>, <a href="http://pubs.acs.org/doi/abs/10.1021/acs.molpharmaceut.7b00346" target="_blank">InsilicoMedicine 2 </a>(DruGAN) (use <a href="https://en.wikipedia.org/wiki/Sci-Hub" target="_blank">Sci-Hub</a> to bypass the paywall), <a href="https://arxiv.org/abs/1711.07839" target="_blank">AstraZeneca 3</a></li><li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank">Recurrent Neural Networks </a>(RNN): <a href="https://openreview.net/forum?id=SkkC41HYl" target="_blank">Paris-Saclay</a>, <a href="https://arxiv.org/abs/1705.04612" target="_blank">Wildcard</a></li><li>Reinforcement Learning (RL)+ RNN: <a href="https://arxiv.org/abs/1611.02796" target="_blank">Google</a>, <a href="http://pubs.acs.org/doi/full/10.1021/acscentsci.7b00512" target="_blank">AstraZeneca 1</a>, <a href="https://arxiv.org/abs/1704.07555" target="_blank">AstraZeneca 2</a>, <a href="https://arxiv.org/abs/1710.00616" target="_blank">University of Tokyo</a>, <a href="https://doi.org/10.1002/minf.201700111" target="_blank">ETH Zurich 1</a>, <a href="https://arxiv.org/abs/1711.10907" target="_blank">University of North Carolina</a>, <a href="https://arxiv.org/abs/1712.07449" target="_blank">Novartis</a>, <a href="http://onlinelibrary.wiley.com/doi/10.1002/minf.201700153/full" target="_blank">ETH Zurich 2</a></li><li>RL+ RNN+ Generative Adversarial Networks (GAN): <a href="https://arxiv.org/abs/1705.10843" target="_blank">Harvard 2</a> (ORGAN), <a href="https://chemrxiv.org/articles/ORGANIC_1_pdf/5309668" target="_blank">Harvard 3</a> (ORGANIC)</li><li>Conditional Graphs: <a href="https://arxiv.org/abs/1801.07299" target="_blank">Peking University</a></li></ul><div>For GAN, there are different flavors: Wasserstein-GAN (<a href="https://arxiv.org/abs/1704.00028" target="_blank">Facebook</a>), Cramer-GAN (<a href="https://arxiv.org/abs/1705.10743" target="_blank">DeepMind</a>), Optimal Transport-GAN (<a href="https://openreview.net/forum?id=rkQkBnJAb" target="_blank">OpenAI</a>), Coulomb-GAN (<a href="https://arxiv.org/abs/1708.08819" target="_blank">Linz University</a>), although at the end, maybe they are all equal (<a href="https://arxiv.org/abs/1711.10337" target="_blank">Google</a>).</div><div>You can also find more in the <b>Natural Language Processing</b> literature (and apply them to <a href="https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system" target="_blank">SMILES</a>):</div><ul><li>Texygen benchmark (<a href="https://arxiv.org/abs/1802.01886v1" target="_blank">Shanghai University</a>)</li><li>MaskGAN (<a href="https://arxiv.org/abs/1801.07736" target="_blank">Google</a>)</li><li>ACtuAL (<a href="https://arxiv.org/abs/1711.04755" target="_blank">University of Montreal</a>)</li><li>ARAE (<a href="https://arxiv.org/abs/1706.04223" target="_blank">New York University</a>)</li><li>Adversarial Generation of Natural Language (<a href="https://arxiv.org/abs/1705.10929" target="_blank">University of Montreal</a>) (and don’t miss the <a href="https://medium.com/@yoav.goldberg/an-adversarial-review-of-adversarial-generation-of-natural-language-409ac3378bd7" target="_blank">adversarial review</a>)</li><li>MaliGAN (<a href="https://arxiv.org/abs/1702.07983" target="_blank">University of Montreal</a>)</li><li>RankGAN (<a href="https://arxiv.org/abs/1705.11001" target="_blank">University of Washington</a>)</li><li>GSGAN (<a href="https://arxiv.org/abs/1611.04051" target="_blank">Alan Turing Institute</a>)</li><li>TextGAN (<a href="https://arxiv.org/abs/1706.03850" target="_blank">Duke University</a>)</li><li>LeakGAN (<a href="https://arxiv.org/abs/1709.08624" target="_blank">Shanghai University</a>)</li></ul><div>Finally, it will be interesting to design a systematic procedure for testing hyperparameter values. These methods are often very sensitive to hyperparameter choice (another suggestion from an anonymous referee of my previous paper).</div>
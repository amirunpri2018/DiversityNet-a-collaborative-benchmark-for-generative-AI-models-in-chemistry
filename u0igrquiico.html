<div>There are many other evaluation metrics, and even evaluations of evaluations metrics (<a href="https://openreview.net/forum?id=Sy1f0e-R-" target="_blank">Cornell</a>).</div><h1 data-label="150350" class="ltx_title_section">Tasks&nbsp;</h1><div>To perform the benchmark, it’s good to start with tasks already done in the literature. Also, it is interesting to evaluate the same model across a large variety of tasks (to avoid overfitting a particular task).</div><div>Multi-objective tasks are more realistic, but more difficult than single-objective tasks (for example, getting molecules which are active, non-toxic, and synthetizable). It has been tried recently (<a href="https://arxiv.org/abs/1801.07299" target="_blank">Peking University</a>).&nbsp;</div><div>Here’s a list of tasks (tell me if I omitted your paper):</div><div>Drug discovery tasks</div><ul><li>Cancer (<a href="http://www.oncotarget.com/index.php?journal=oncotarget&amp;page=article&amp;op=view&amp;path%5B%5D=14073&amp;path%5B%5D=44886" target="_blank">In SilicoMedicine 1</a> and <a href="http://pubs.acs.org/doi/abs/10.1021/acs.molpharmaceut.7b00346" target="_blank">InsilicoMedicine 2</a> (use <a href="https://en.wikipedia.org/wiki/Sci-Hub" target="_blank">Sci-Hub</a> to bypass the paywall))</li><li>Targeting the <a href="https://en.wikipedia.org/wiki/5-HT2A_receptor" target="_blank">5-HT2A Receptor</a> (<a href="https://en.wikipedia.org/wiki/Selective_serotonin_reuptake_inhibitor" target="_blank">antidepressants</a>), <a href="https://en.wikipedia.org/wiki/Malaria" target="_blank">Malaria</a>, <a href="https://en.wikipedia.org/wiki/Staphylococcus_aureus" target="_blank">staph aureus</a> (<a href="http://pubs.acs.org/doi/full/10.1021/acscentsci.7b00512" target="_blank">AstraZeneca 1</a>)</li><li>Activity on the <a href="https://en.wikipedia.org/wiki/Dopamine_receptor_D2" target="_blank">Dopamine receptor D2</a>: <a href="https://en.wikipedia.org/wiki/Antipsychotic" target="_blank">antipsychotics</a> (<a href="https://arxiv.org/abs/1704.07555" target="_blank">AstraZeneca 2</a>, <a href="https://arxiv.org/abs/1711.07839" target="_blank">AstraZeneca 3</a>)</li><li>Activity on <a href="https://en.wikipedia.org/wiki/PPAR_agonist" target="_blank">PPAR</a> and <a href="https://en.wikipedia.org/wiki/Retinoid_X_receptor" target="_blank">RXR</a>: lowers <a href="https://en.wikipedia.org/wiki/Triglycerides" target="_blank">triglycerides</a> and <a href="https://en.wikipedia.org/wiki/Blood_sugar" target="_blank">blood sugar</a> (<a href="https://doi.org/10.1002/minf.201700111" target="_blank">ETH Zurich 1</a>, <a href="http://onlinelibrary.wiley.com/doi/10.1002/minf.201700153/full" target="_blank">ETH Zurich 2</a>).</li><li><a href="https://en.wikipedia.org/wiki/Janus_kinase_inhibitor" target="_blank">Inhibition of JAK2</a>: cancer, <a href="https://en.wikipedia.org/wiki/Inflammatory_disease" target="_blank">inflammatory diseases</a>, various skin conditions, and <a href="https://en.wikipedia.org/wiki/Autoimmune_disease" target="_blank">autoimmune diseases</a> (<a href="https://arxiv.org/abs/1711.10907" target="_blank">University of North Carolina</a>)</li><li>Joint inhibition of <a href="https://en.wikipedia.org/wiki/MAPK10" target="_blank">JNK3</a> and <a href="https://en.wikipedia.org/wiki/GSK3B" target="_blank">GSK3β</a>: <a href="https://en.wikipedia.org/wiki/Alzheimer%27s_disease" target="_blank">Alzheimer disease</a> (<a href="https://arxiv.org/abs/1801.07299" target="_blank">Peking University</a>)</li></ul><div>Organic materials tasks</div><ul><li><a href="https://en.wikipedia.org/wiki/Organic_solar_cell" target="_blank">Organic solar cells</a>: <a href="http://web.anl.gov/pse/solar/research/photovoltaic/opv_pce.html" target="_blank">Power Conversion Efficiency</a> (<a href="https://chemrxiv.org/articles/ORGANIC_1_pdf/5309668" target="_blank">Harvard 3</a>)</li><li><a href="https://en.wikipedia.org/wiki/Organic_semiconductor" target="_blank">Organic semi-conductors</a>: <a href="https://en.wikipedia.org/wiki/HOMO/LUMO" target="_blank">HOMO–LUMO gap</a> <b>(</b><a href="https://github.com/tsudalab/ChemTS" target="_blank">University of Tokyo</a>,&nbsp;<a href="http://onlinelibrary.wiley.com/doi/10.1002/minf.201700133/full" target="_blank">Denmark Tech</a>&nbsp;(use&nbsp;<a href="https://en.wikipedia.org/wiki/Sci-Hub" target="_blank">Sci-Hub</a>&nbsp;for the paywall)<b>)</b></li></ul><div>Participants can also propose their own favorite objectives. In any case, I think it is better to consider at least one specific real-world application, and not just generate ‘drug-like’ molecules, as in those preliminary papers by <a href="http://pubs.acs.org/doi/abs/10.1021/acscentsci.7b00572" target="_blank">Harvard 1</a>, <a href="https://arxiv.org/abs/1611.02796" target="_blank">Google/Cambridge</a>, <a href="https://openreview.net/forum?id=SkkC41HYl" target="_blank">Paris-Saclay</a>,  <a href="https://arxiv.org/abs/1705.04612" target="_blank">Wildcard</a>, <a href="https://arxiv.org/abs/1705.10843" target="_blank">Harvard 2</a>, <a href="https://arxiv.org/abs/1712.07449" target="_blank">Novartis</a>, <a href="https://openreview.net/forum?id=SyqShMZRb" target="_blank">Georgia Tech</a>.</div><h1 data-label="179191" class="ltx_title_section">Data</h1><div>This DiversityNet benchmark is based on publicly available data, like all the papers cited above. In most papers, data is taken from:</div><ul><li><a href="https://pubchem.ncbi.nlm.nih.gov/" target="_blank">PubChem</a></li><li><a href="https://www.ebi.ac.uk/chembldb/" target="_blank">ChEMBL</a></li><li>&nbsp;<a href="https://solr.ideaconsult.net/search/excape/" target="_blank">ExCAPE-DB</a>, which aggregates PubChem and ChEMBL.</li><li><a href="http://zinc15.docking.org/" target="_blank">ZINC</a></li></ul><div>Many papers only use small datasets (including <a href="https://arxiv.org/abs/1708.08227" target="_blank">mine</a>), and in some way, that’s bad. Model pre-training should be made on a large dataset. </div><div>Even better, different pre-training set sizes could be tested (5K, 10K, 15K, 30K, 50K, 100K, 250K, 1M) to understand how performance of the generative model changes (that’s a suggestion from an anonymous referee of my paper).</div><div>Besides small molecules chemistry, the same generative models can be used for other tasks related to drug discovery: for RNA sequences (<a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1882-7" target="_blank">University of Tokyo</a>), for DNA sequences (<a href="https://arxiv.org/abs/1712.06148" target="_blank">University of Toronto</a>) and for <a href="https://en.wikipedia.org/wiki/Peptide" target="_blank">peptides</a> (<a href="http://pubs.acs.org/doi/10.1021/acs.jcim.7b00414" target="_blank">ETH Zurich 3</a>). However, I think it is better to keep those non-chemistry tasks for separate benchmarks: DiversityNet-genetics and DiversityNet-peptides.&nbsp;</div><h1 data-label="714352" class="ltx_title_section">Generative models</h1><div>It’s good to start with models already tried in the literature:</div><ul><li><a href="http://kvfrans.com/variational-autoencoders-explained/" target="_blank">Variational auto-encoder</a>: <a href="http://pubs.acs.org/doi/abs/10.1021/acscentsci.7b00572" target="_blank">Harvard 1</a>, <a href="https://arxiv.org/abs/1703.01925" target="_blank">Alan Turing Institute</a>, <a href="https://arxiv.org/abs/1711.07839" target="_blank">AstraZeneca 3</a>, <a href="https://openreview.net/forum?id=SyqShMZRb" target="_blank">Georgia Tech</a></li><li><a href="https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4" target="_blank">Adversarial auto-encoder</a>: <a href="http://www.oncotarget.com/index.php?journal=oncotarget&amp;page=article&amp;op=view&amp;path%5B%5D=14073&amp;path%5B%5D=44886" target="_blank">In SilicoMedicine 1</a>, <a href="http://pubs.acs.org/doi/abs/10.1021/acs.molpharmaceut.7b00346" target="_blank">InsilicoMedicine 2 </a>(DruGAN) (use <a href="https://en.wikipedia.org/wiki/Sci-Hub" target="_blank">Sci-Hub</a> to bypass the paywall), <a href="https://arxiv.org/abs/1711.07839" target="_blank">AstraZeneca 3</a></li><li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank">Recurrent Neural Networks </a>(RNN): <a href="https://openreview.net/forum?id=SkkC41HYl" target="_blank">Paris-Saclay</a>, <a href="https://arxiv.org/abs/1705.04612" target="_blank">Wildcard</a></li><li>Reinforcement Learning (RL)+ RNN: <a href="https://arxiv.org/abs/1611.02796" target="_blank">Google</a>, <a href="http://pubs.acs.org/doi/full/10.1021/acscentsci.7b00512" target="_blank">AstraZeneca 1</a>, <a href="https://arxiv.org/abs/1704.07555" target="_blank">AstraZeneca 2</a>, <a href="https://arxiv.org/abs/1710.00616" target="_blank">University of Tokyo</a>, <a href="https://doi.org/10.1002/minf.201700111" target="_blank">ETH Zurich 1</a>, <a href="https://arxiv.org/abs/1711.10907" target="_blank">University of North Carolina</a>, <a href="https://arxiv.org/abs/1712.07449" target="_blank">Novartis</a>, <a href="http://onlinelibrary.wiley.com/doi/10.1002/minf.201700153/full" target="_blank">ETH Zurich 2</a></li><li>RL+ RNN+ Generative Adversarial Networks (GAN): <a href="https://arxiv.org/abs/1705.10843" target="_blank">Harvard 2</a> (ORGAN), <a href="https://chemrxiv.org/articles/ORGANIC_1_pdf/5309668" target="_blank">Harvard 3</a> (ORGANIC)</li><li>Conditional Graphs: <a href="https://arxiv.org/abs/1801.07299" target="_blank">Peking University</a></li></ul><div>For GAN, there are different flavors: Wasserstein-GAN (<a href="https://arxiv.org/abs/1704.00028" target="_blank">Facebook</a>), Cramer-GAN (<a href="https://arxiv.org/abs/1705.10743" target="_blank">DeepMind</a>), Optimal Transport-GAN (<a href="https://openreview.net/forum?id=rkQkBnJAb" target="_blank">OpenAI</a>), Coulomb-GAN (<a href="https://arxiv.org/abs/1708.08819" target="_blank">Linz University</a>), although at the end, maybe they are all equal (<a href="https://arxiv.org/abs/1711.10337" target="_blank">Google</a>).</div><div>You can also find more in the <b>Natural Language Processing</b> literature (and apply them to <a href="https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system" target="_blank">SMILES</a>):</div><ul><li>Texygen benchmark (<a href="https://arxiv.org/abs/1802.01886v1" target="_blank">Shanghai University</a>)</li><li>MaskGAN (<a href="https://arxiv.org/abs/1801.07736" target="_blank">Google</a>)</li><li>ACtuAL (<a href="https://arxiv.org/abs/1711.04755" target="_blank">University of Montreal</a>)</li><li>ARAE (<a href="https://arxiv.org/abs/1706.04223" target="_blank">New York University</a>)</li><li>Adversarial Generation of Natural Language (<a href="https://arxiv.org/abs/1705.10929" target="_blank">University of Montreal</a>) (and don’t miss the <a href="https://medium.com/@yoav.goldberg/an-adversarial-review-of-adversarial-generation-of-natural-language-409ac3378bd7" target="_blank">adversarial review</a>)</li><li>MaliGAN (<a href="https://arxiv.org/abs/1702.07983" target="_blank">University of Montreal</a>)</li><li>RankGAN (<a href="https://arxiv.org/abs/1705.11001" target="_blank">University of Washington</a>)</li><li>GSGAN (<a href="https://arxiv.org/abs/1611.04051" target="_blank">Alan Turing Institute</a>)</li><li>TextGAN (<a href="https://arxiv.org/abs/1706.03850" target="_blank">Duke University</a>)</li><li>LeakGAN (<a href="https://arxiv.org/abs/1709.08624" target="_blank">Shanghai University</a>)</li></ul><div>Finally, it will be interesting to design a systematic procedure for testing hyperparameter values. These methods are often very sensitive to hyperparameter choice (another suggestion from an anonymous referee of my previous paper).</div>
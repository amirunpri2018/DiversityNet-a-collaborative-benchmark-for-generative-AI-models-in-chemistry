<div>In this case, another measure of diversity is better: <a href="https://www.quora.com/Which-is-the-best-measure-of-uncertainty-variance-or-entropy-or-are-they-both-equivalent/answer/Suresh-Kondamudi" target="_blank"><b>entropy</b></a>.&nbsp;</div><div>To see why, it helps looking at the simplified case when data labels are discrete (like in classification situations). In this case, entropy is higher when data is spread in a lot of categories: for <i>N</i> equi-distributed categories, the entropy is equal to log(<i>N</i>), which is an increasing function of <i>N</i>.</div><div>This reasoning can be generalized to our setting, where data  lives in a continuous and high-dimensional space, using&nbsp;<a href="https://en.m.wikipedia.org/wiki/Limiting_density_of_discrete_points" target="_blank">differential entropy</a>:</div><div></div><div>Let&nbsp;</div><div></div><h2 data-label="782118" class="ltx_title_subsection">Earth Mover Distance with a reference dataset</h2><div>Another measure of internal diversity is to compare the set of generated samples with a reference set, which is known to be diverse <i>a priori</i>. For example, the <a href="http://zinc15.docking.org/" target="_blank">ZINC</a> dataset seems suitable. Chemists can propose alternative reference datasets.&nbsp;</div><div>The idea is to take a random subset of the reference set with the same size as the generated set. Then to consider those two sets as two piles of sand in the chemical space, and measure the energy necessary to move the first pile into the second pile (this measure is known as <a href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance" target="_blank">Earth Mover Distance</a> in statistics, and <a href="https://en.wikipedia.org/wiki/Wasserstein_metric" target="_blank">Wasserstein metric</a> in mathematics).</div>
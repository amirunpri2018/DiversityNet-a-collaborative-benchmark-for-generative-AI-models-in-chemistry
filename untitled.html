<div><i>Making Commenting the document is possible without account, but for editing, you need to:</i></div><ul><li><i>Create an Authorea account</i></li><li><i>Join the DiversityNet group</i></li></ul><div></div><div>Generative AI models in chemistry are increasingly popular in the research community. They have applications in drug discovery and organic materials (solar cells, semi-conductors). Their goal is to generate virtual molecules with desired chemical properties (more details in this&nbsp;<a href="https://medium.com/the-ai-lab/chemgan-challenge-for-drug-discovery-can-ai-reproduce-natural-chemical-diversity-8f1f2528ee22" target="_blank">blog post</a>).&nbsp;</div><div>However, this flourishing literature still lacks a unified benchmark. Such benchmark would provide a common framework to evaluate and <b>compare </b>different generative models. Moreover, this would allow to formulate <b>best practices</b> for this emerging industry of ‘AI molecule generators’: how much training data is needed, for how long the model should be trained, and so on.</div><div>That’s what the<b> DiversityNet benchmark</b> is about. DiversityNet continues the tradition of data science benchmarks, after the MoleculeNet (<a href="https://arxiv.org/abs/1703.00564" target="_blank">Stanford</a>) benchmark for predictive models in chemistry, and the ImageNet challenge (<a href="https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#" target="_blank">Stanford</a>) in computer vision.</div><div></div>